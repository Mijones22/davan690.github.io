---
title: "Pre-submission"
subtitle: "What is it all about?"
layout: post
tags: ["test", "tools","rmd", "rstudio","home", "presub"]
image: /img/osf-image.jpg
permlink: /osf-pre-rego.html
---

Pre-submission has always been a concept in Science. The idea that we are all bias as researchers should hit home for all researchers and the tools to allow our research to be reproducible and un-bias should be at the forefront of our box of tools however this is not the case prior to recent developments in part to [this](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970) Nature article on Scientific reproducibility.

Thankfully, a collection of open-minded scientists have created and mantained a `osf` database with the tools to allow multidisplinary collabouration and reproducibility in science.

You can create a preregistration by choosing OSF Preregistration from the list of registration forms when registering a project of the `osf` webpage or from preregistration landing page. The resources below will walk you through how to start a new preregistration. Many of the resources are extended from the `osf` [webpage and associated material](http://help.osf.io/m/registrations/l/546603-create-a-preregistration).

## Benefits

The benefits of preregistration depend directly on what kind of information, and how much of it, is included inside that preregistration. A very sparse outline of a study plan may be sufficient to increase the discoverability of the research and thus help to address the file drawer effect (Rosenthal, 1979; Franco, Malhotra, & Simonovits, 2014), yet insufficient to assist in evaluating claims resulting from that research. Including a detailed analysis plan in the preregistration may additionally help reduce unintentional false positive inflation of results (Forstmeier, Wagenmakers, & Parker, 2017) and better enable readers to distinguish exploratory from hypothesis-testing elements in a study (Nosek, Ebersole, DeHaven, & Mellor, 2018). Both modes of research are essential for science to advance, but presenting the results of data-dependent, exploratory discoveries using the tools of statistical inference designed for confirmatory studies makes the results appear more surprising, and publishable, which comes at the expense of their credibility (Nosek, Spies, & Motyl, 2012).

While we recommend preregistering all types of research, the most benefits accrue when performing hypothesis testing, confirmatory, research and these benefits are of particular importance to addressing issues of reproducibility in the published literature (Munafò et al., 2017). Our strongest recommendation is therefore to preregister confirmatory research and to include a detailed analysis plan in that preregistration. If setting out on purely exploratory research or pilot studies, preregistration can still help you remember that intention at the end of that project, improve the transparency of your research

## Timing

Found [here](https://cos.io/blog/preregistration-plan-not-prison/?_ga=2.232172375.996847213.1560227958-1887854903.1554694868)

### Example 1: 
#### Prior to Data Collection

This one is easy. If you want to make a change to your research plan before you’ve begun data collection, you can start a new preregistration in the same OSF project with the updated information. You can either withdraw the first one, or keep it to reference in the second, new preregistration.

### Example 2
#### During Data Collection

This phase gets to some of the core questions about making a preregistration, the biases it is designed to address, and the role of transparency in achieving the overall goal. For the Preregistration Challenge, we permit researchers to create these research plans up to the point of data analysis, which includes calculating summary statistics in a dataset. After that point, there is too much room for bias to creep in. The data being used to test the hypothesis would have also been used to create the hypothesis test, thus making confirmatory and exploratory work harder to distinguish.

Given the above rationale, you are free and encouraged to make a new preregistration up to the point of data analysis. We recommend that you preserve the original preregistration, include a link to it in the new preregistration, and specify the changes that you are making and why. Is there still room for bias to creep in? Of course there is. But by maintaining the trail of ideas, you preserve the ability to evaluate that very question of bias. Every preregistration must include a statement that describes the degree to which data exist, thereby allowing for transparency into this part of the research process.

One deviation that may occur during this stage is a smaller than expected sample size. If you don’t have full control over your sample size, your preregistration must have a stopping rule that will dictate when to stop collecting data. The purpose of this rule is avoid continued rounds of data collection in an attempt to find statistically significant results. This repeated measuring and testing dramatically inflates the likelihood of seeing a significant finding and makes the resulting p-values meaningless, especially if they are not all reported. However, being unable to recruit as many subjects as planned is often totally innocuous. This lower than expected sample size may come after exhaustive efforts to recruit and not after peeking at the incoming data, in which case transparently reporting your efforts will allow others to judge the validity of the reported tests.

### Example 3
#### After Data Analysis Has Begun

After data analysis has begun, it becomes nearly impossible to determine if decisions are justifiable and are outcome-independent, as the outcome is now known. The short answer for making changes or finding “better” analyses after data collection is to simply label any new analyses as what they are: data-dependent, exploratory analyses. Such analyses are an important feature of scientific research. Without a puzzling new finding, discoveries wouldn’t be made. If we don’t value the results of exploratory research, those who have never preregistered an analysis could rightly claim that preregistration will stymie future discoveries by limiting us to only reaffirming what is known. So go ahead, do whatever you want with the data as it comes in, but make sure you conduct and report the originally specified analyses, as those were your strongest predictions before running the experiment.

That being said, there are exceptions to every rule. From time to time, it really would be ineffective to stick to an outdated plan. If the results are uninterpretable because the data do not meet the assumptions of the specified tests, state that. State the plan, the reason for the change, provide a link to the outdated analyses so that the reader can evaluate the assertion, and then move on with the appropriate test. Such situations are difficult to address in a generic way, but the guiding principle for any such decisions should be to be transparent. Transparency allows the conversation to be meaningful.

If you are worried about harming the publishability of your work once you have deviated from the original plan, focus on journals that have made a commitment to rewarding transparency, those that have signed the Transparency and Openness Promotion Guidelines, those that issue Open Practice Badges, or those that accept Registered Reports. These practices signal that the journals have a core commitment toward open and reproducible research and so are best suited to evaluating work based on ideal scientific practices.

Another thing to consider when making a change to the preregistration is making changes to the OSF project that the preregistration is attached to. Though it is not frozen and timestamped, it does create a log of the changes made. This change should still be addressed in the final manuscript, but it is also helpful to have that extra evidence.

In all of the above recommendations, the central theme is to be as transparent as possible. There are likely many other solutions to the problems discussed above. The question one should ask when deciding to make a change is “how can I be as transparent as possible so that my audience will know exactly why I am doing what I am doing?” If you can satisfactorily answer this question, your reviewers and readership should have no problem accepting your changes.

## Seperating Confirmatory and Exploratory Research

Preregistration allows the researcher to make a clear distinction between both modes of research. And there are even badges for the extend of research studies [here](https://cos.io/our-services/open-science-badges/). And the evidence for this is [here](https://osf.io/tvyxz/wiki/2.%20Awarding%20Badges/?_ga=2.260517133.1420078275.1525996125-902006626.1525996125).

### Confirmatory Research

- Hypothesis testing
- Results are held to the highest standards
- Data-independent
- Minimizes false positives
- P-values retain diagnostic value
- Inferences may be drawn to wider population

### Exploratory Research

- Hypothesis generating
- Results deserve to be replicated and confirmed
- Data-dependent
- Minimizes false negatives in order to find unexpected discoveries
- P-values lose diagnostic value
- Not useful for making inferences to any wider population

## Tutorials

- [Pre-registration tutorial](http://help.osf.io/m/registrations/l/546603-create-a-preregistration)
- [Design like its 2019](https://cos.io/prereg/?_ga=2.232172375.996847213.1560227958-1887854903.1554694868)
- [What is pre-rego](https://cos.io/prereg/?_ga=2.232172375.996847213.1560227958-1887854903.1554694868)
- [Its not a prison](https://cos.io/blog/preregistration-plan-not-prison/?_ga=2.232172375.996847213.1560227958-1887854903.1554694868)
- [Open-science framework tools for TREE](https://osf.io/g65cb/)

## Resources

Training services [here](https://cos.io/our-services/training-services/)

[Example in Tropical Biology](https://osf.io/ps8dc/)

Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638 

Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484

Forstmeier, W., Wagenmakers, E.-J., & Parker, T. H. (2017). Detecting and avoiding likely false-positive findings – a practical guide. Biological Reviews, 92(4), 1941–1968. https://doi.org/10.1111/brv.12315

Nosek, Brian A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114

Nosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058

Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Percie du Sert, N., … Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 0021. https://doi.org/10.1038/s41562-016-0021

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632